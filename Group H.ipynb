{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61442bb3",
   "metadata": {},
   "source": [
    "# Impoting Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c531bb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 78>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier, export_graphviz\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n\u001b[0;32m     80\u001b[0m sns\u001b[38;5;241m.\u001b[39mset()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "#import necessary libraries\n",
    "#!pip install sas7bdat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Remember: library imports are ALWAYS at the top of the script, no exceptions!\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# notice we are adding two new imports for visualizations; This was not here last class\n",
    "from itertools import product\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "\n",
    "# Seeting seaborn style\n",
    "sns.set()\n",
    "# Remember: library imports are ALWAYS at the top of the script, no exceptions!\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sns.set()\n",
    "\n",
    "from collections import Counter\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcf6c1",
   "metadata": {},
   "source": [
    "# Context\n",
    "Business Situation\n",
    "A2Z Insurance (A2Z) is a portuguese long standing insurance company that serves a wide array of\n",
    "insurance services: Motor, Household, Health, Life and Work Compensation. Although A2Z primarily\n",
    "serves portuguese customers, a significant portion of their customer acquisition comes from their web\n",
    "site. Customers can sign up to A2Z services through their branches, by telephone, or on the web site.\n",
    "In 2016, A2Z became one of the largest insurers in Portugal. However, the lack of a data driven\n",
    "culture in the company ultimately led to poorly maintained databases over the years. A2Z is trying to\n",
    "make better use of the database it has regarding its customers. So far, it has simply mass-marketed\n",
    "everything. All potential and existing customers get the same promotions, and there are no attempts to\n",
    "identify target markets for cross-selling opportunities. Now, A2Z wants start differentiating customers,\n",
    "and developing more focused programs.\n",
    "A2Z provided you an ABT (Analytic Based Table) with data regarding a sample of 10.290 Customers\n",
    "from its active database. These are customers that had at least one insurance service with the company\n",
    "at the time the dataset was extracted. Your job is to segment the database and find the relevant\n",
    "clusters of customers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3138a90",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a2z_insurance.sas7bdat dataset\n",
    "data = pd.read_sas(\"C:/Users/Evans/Desktop/Data Mining Group Work/Group Work Materials-20221111/a2z_insurance.sas7bdat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fba2cbe",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bf6bb",
   "metadata": {},
   "source": [
    "# Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d83fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(['CustID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636d4dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 5 records in the dataset\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f71f41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dataset data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aefd02",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#count of missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c17536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for duplicated records\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9e589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407a60fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdb53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EducDeg'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de642108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\"\", np.nan, inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99364d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#heck dataset data types again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f27dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix wrong dtypes\n",
    "df.Children = data.Children.astype(\"boolean\")  # converting to \"boolean\" over \"bool\" allows preservation of NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8161e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check descriptive statistics again\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ef0c5",
   "metadata": {},
   "source": [
    "# Visual Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d25e66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define metric and non-metric features. Why?\n",
    "non_metric_features = [\"Children\", \"EducDeg\", \"GeoLivArea\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ddb5e",
   "metadata": {},
   "source": [
    "## Numeric Variables' Univariate Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56791f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Metric Variable Histogram\n",
    "plt.hist(df[\"PremMotor\"], bins=10) \n",
    "plt.title(\"PremMotor Histogram\", y=-0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Metric Variable Box Plot\n",
    "sns.boxplot(y=df[\"BirthYear\"])\n",
    "plt.title(\"BirthYear Box Plot\", y=-0.2)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd831a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Numeric Variables' Histograms in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each histogram will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each histogram (hint: use the ax.hist() instead of plt.hist()):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    ax.hist(df[feat])\n",
    "    ax.set_title(feat, y=-0.13)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables' Histograms\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'numeric_variables_histograms.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cfe70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All Numeric Variables' Box Plots in one figure\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each box plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each box plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), metric_features): # Notice the zip() function and flatten() method\n",
    "    sns.boxplot(x=df[feat], ax=ax)\n",
    "    \n",
    "# Layout\n",
    "# Add a centered title to the figure:\n",
    "title = \"Numeric Variables' Box Plots\"\n",
    "\n",
    "plt.suptitle(title)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'numeric_variables_boxplots.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a006d50",
   "metadata": {},
   "source": [
    "## Pairwise Relationship of Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62fbd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Relationship of Numerical Variables\n",
    "sns.set()\n",
    "\n",
    "# Setting pairplot\n",
    "sns.pairplot(df[metric_features], diag_kind=\"hist\")\n",
    "\n",
    "# Layout\n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.suptitle(\"Pairwise Relationship of Numerical Variables\", fontsize=20)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'pairwise_relationship_of_numerical_variables.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f4772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Non-Metric variable bar plot\n",
    "sns.set() # this resets our formatting defaults\n",
    "sns.countplot(x=df[\"EducDeg\"])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bda9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Non-Metric Variables' Absolute Frequencies\n",
    "sns.set()\n",
    "\n",
    "# Prepare figure. Create individual axes where each bar plot will be placed\n",
    "fig, axes = plt.subplots(2, ceil(len(non_metric_features) / 2), figsize=(20, 11))\n",
    "\n",
    "# Plot data\n",
    "# Iterate across axes objects and associate each bar plot (hint: use the ax argument):\n",
    "for ax, feat in zip(axes.flatten(), non_metric_features): # Notice the zip() function and flatten() method\n",
    "    sns.countplot(x=df[feat].astype(object), ax=ax, color='#007acc')\n",
    "\n",
    "title = \"Categorical/Low Cardinality Variables' Absolute Frequencies\"\n",
    "plt.suptitle(title)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'categorical_variables_frequecies.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ce58f",
   "metadata": {},
   "source": [
    "## Metric Variables' Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f155230",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prepare figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Obtain correlation matrix. Round the values to 2 decimal cases. Use the DataFrame corr() and round() method.\n",
    "corr = np.round(df[metric_features].corr(method=\"pearson\"), decimals=2)\n",
    "\n",
    "# Build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "# Layout\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "#plt.savefig(os.path.join('..', 'figures', 'correlation_matrix.png'), dpi=200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcd70bb",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f56f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \"\" by nans\n",
    "df.replace(\"\", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cab773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check dataset data types again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c829d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check descriptive statistics again\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532639be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define metric and non-metric features. Why?\n",
    "non_metric_features = [\"Children\", \"EducDeg\", \"GeoLivArea\"]\n",
    "metric_features = df.columns.drop(non_metric_features).to_list()\n",
    "#metric_features.\n",
    "#metric_features = df[df.columns.drop(non_metric_features)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad4e57",
   "metadata": {},
   "source": [
    "## Fill missing values (Data imputation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820df42e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1848bbab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dropping duplicate records\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', np.round(df.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['BirthYear']>=1935]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477786a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', np.round(df.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402da5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['FirstPolYear']<=1998]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f1998",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of data kept after removing outliers:', np.round(df.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['FirstPolYear'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe709529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88369164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy to apply central tendency measures imputation\n",
    "df_central = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e240a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of missing values\n",
    "df_central.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb96e27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modes = df_central[non_metric_features].mode().loc[0]\n",
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new df copy to explore neighbordhood imputation\n",
    "df_neighbors = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57930f3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seeing rows with NaNs\n",
    "nans_index = df_neighbors.isna().any(axis=1)\n",
    "df_neighbors[nans_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe6caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNImputer - only works for numerical variables\n",
    "imputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "df_neighbors[metric_features] = imputer.fit_transform(df_neighbors[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283622da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See rows with NaNs imputed\n",
    "df_neighbors.loc[nans_index, metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6036b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's keep the central imputation\n",
    "df = df_central.copy()\n",
    "df.isna().sum()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc9ab9a",
   "metadata": {},
   "source": [
    "## Outlier removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c5bb4",
   "metadata": {},
   "source": [
    "### Outlier removal using only the IQR method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54567738",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1169aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "q25 = df.quantile(.5).astype(int)\n",
    "q75 = df.quantile(.95).astype(int)\n",
    "iqr = (q75 - q25)\n",
    "\n",
    "upper_lim = q75 + 1.5 * iqr\n",
    "lower_lim = q25 - 1.5 * iqr\n",
    "\n",
    "filters2 = []\n",
    "for metric in metric_features:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters2.append(df[metric].between(llim, ulim, inclusive=True))\n",
    "\n",
    "filters2 = pd.Series(np.all(filters2, 0))\n",
    "df_2 = df[filters2]\n",
    "print('Percentage of data kept after removing outliers:', np.round(df_2.shape[0] / df_original.shape[0], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b88dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea739554",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'] = datetime.now().year - df['BirthYear']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d86656",
   "metadata": {},
   "source": [
    "## Variable selection: Redundancy VS Relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables according to their correlations\n",
    "df.drop(columns=['BirthYear'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6f6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating metric_features\n",
    "metric_features.append(\"Age\")\n",
    "#metric_features.remove(\"mnt\")\n",
    "#metric_features.remove(\"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55531c",
   "metadata": {},
   "source": [
    "### Relevancy\n",
    "Selecting variables based on the relevancy of each one to the task. Example: remove uncorrelated variables with the target, stepwise regression, use variables for product clustering, use variables for socio-demographic clustering, ...\n",
    "\n",
    "Variables that aren't correlated with any other variable are often also not relevant. In this case we will not focus on this a lot since we don't have a defined task yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390f954",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e9161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847af5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the data\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feat = scaler.fit_transform(df_minmax[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef064394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\", scaler.data_min_, \"\\n\", scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fde392",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_minmax[metric_features] = scaled_feat\n",
    "df_minmax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaf3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking max and min of minmaxed variables\n",
    "df_minmax[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16869d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(df_standard[metric_features])\n",
    "scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac18b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what the fit method is doing (notice the trailing underscore):\n",
    "print(\"Parameters fitted:\\n\", scaler.mean_, \"\\n\", scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f8905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standard[metric_features] = scaled_feat\n",
    "df_standard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking mean and variance of standardized variables\n",
    "df_standard[metric_features].describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_standard.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf27b4",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ohc = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d740f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OneHotEncoder to encode the categorical features. Get feature names and create a DataFrame \n",
    "# with the one-hot encoded categorical features (pass feature names)\n",
    "ohc = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "ohc_feat = ohc.fit_transform(df_ohc[non_metric_features])\n",
    "ohc_feat_names = ohc.get_feature_names_out()\n",
    "ohc_df = pd.DataFrame(ohc_feat, index=df_ohc.index, columns=ohc_feat_names)  # Why the index=df_ohc.index?\n",
    "ohc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f03184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain ohc variables\n",
    "df_ohc = pd.concat([df_ohc.drop(columns=non_metric_features), ohc_df], axis=1)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775432a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_ohc.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19aad90",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae5ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce dimensionality of data\n",
    "pca = PCA()\n",
    "pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "pca_feat  # What is this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output PCA table\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d05e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1a0c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA again with the number of principal components you want to retain\n",
    "pca = PCA(n_components=4)\n",
    "pca_feat = pca.fit_transform(df_pca[metric_features])\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(pca.n_components_)]\n",
    "pca_df = pd.DataFrame(pca_feat, index=df_pca.index, columns=pca_feat_names)  # remember index=df_pca.index\n",
    "pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee56c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassigning df to contain pca variables\n",
    "df_pca = pd.concat([df_pca, pca_df], axis=1)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe68be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _color_red_or_green(val):\n",
    "    if val < -0.45:\n",
    "        color = 'background-color: red'\n",
    "    elif val > 0.45:\n",
    "        color = 'background-color: green'\n",
    "    else:\n",
    "        color = ''\n",
    "    return color\n",
    "\n",
    "# Interpreting each Principal Component\n",
    "loadings = df_pca[metric_features + pca_feat_names].corr().loc[metric_features, pca_feat_names]\n",
    "loadings.style.applymap(_color_red_or_green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_pca.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538840d",
   "metadata": {},
   "source": [
    "**Some final data preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this after checking the new pandas profiling report\n",
    "df.drop(columns=['PC3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12bf09",
   "metadata": {},
   "source": [
    "## Rename OHE columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53bc178",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename OHE columns from \"feature_val_a\" to \"x_feature_val_a\"\n",
    "## We do this to be able to distinguish the OHE columns more easily later\n",
    "\n",
    "## Assemble OHE columns and their new column names\n",
    "rename_ohe_cols = {}\n",
    "\n",
    "for i in non_metric_features:\n",
    "    for j in df.columns[df.columns.str.startswith(i)].to_list() :\n",
    "        rename_ohe_cols[j] = 'x_' + j\n",
    "\n",
    "df.rename(columns=rename_ohe_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48f41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3076a",
   "metadata": {},
   "source": [
    "## Redo data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b34a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5467d8d2",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54678c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting feature names into groups\n",
    "non_metric_features = df.columns[df.columns.str.startswith('x')]\n",
    "pc_features = df.columns[df.columns.str.startswith('PC')]\n",
    "metric_features = df.columns[~df.columns.str.startswith('x') & ~df.columns.str.startswith('PC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a41285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing HC\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters=5)\n",
    "hc_labels = hclust.fit_predict(df[metric_features])\n",
    "hc_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66666c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the clusters\n",
    "df_concat = pd.concat((df, pd.Series(hc_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing SST\n",
    "X = df[metric_features].values\n",
    "sst = np.sum(np.square(X - X.mean(axis=0)), axis=0)\n",
    "\n",
    "# Computing SSW\n",
    "ssw_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssw_iter.append(np.sum(np.square(X_k - X_k.mean(axis=0)), axis=0))\n",
    "ssw = np.sum(ssw_iter, axis=0)\n",
    "\n",
    "# Computing SSB\n",
    "ssb_iter = []\n",
    "for i in np.unique(hc_labels):\n",
    "    X_k = X[hc_labels == i]\n",
    "    ssb_iter.append(X_k.shape[0] * np.square(X_k.mean(axis=0) - X.mean(axis=0)))\n",
    "ssb = np.sum(ssb_iter, axis=0)\n",
    "\n",
    "# Verifying the formula\n",
    "np.round(sst) == np.round((ssw + ssb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025bd1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"This function computes the R2 for a set of cluster solutions given by the application of a hierarchical method.\n",
    "    The R2 is a measure of the homogenity of a cluster solution. It is based on SSt = SSw + SSb and R2 = SSb/SSt. \n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Dataset to apply clustering\n",
    "    link_method (str): either \"ward\", \"complete\", \"average\", \"single\"\n",
    "    max_nclus (int): maximum number of clusters to compare the methods\n",
    "    min_nclus (int): minimum number of clusters to compare the methods. Defaults to 1.\n",
    "    dist (str): distance to use to compute the clustering solution. Must be a valid distance. Defaults to \"euclidean\".\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: R2 values for the range of cluster solutions\n",
    "    \"\"\"\n",
    "    def get_ss(df):\n",
    "        ss = np.sum(df.var() * (df.count() - 1))\n",
    "        return ss  # return sum of sum of squares of each df variable\n",
    "    \n",
    "    sst = get_ss(df)  # get total sum of squares\n",
    "    \n",
    "    r2 = []  # where we will store the R2 metrics for each cluster solution\n",
    "    \n",
    "    for i in range(min_nclus, max_nclus+1):  # iterate over desired ncluster range\n",
    "        cluster = AgglomerativeClustering(n_clusters=i, affinity=dist, linkage=link_method)\n",
    "        \n",
    "        \n",
    "        hclabels = cluster.fit_predict(df) #get cluster labels\n",
    "        \n",
    "        \n",
    "        df_concat = pd.concat((df, pd.Series(hclabels, name='labels')), axis=1)  # concat df with labels\n",
    "        \n",
    "        \n",
    "        ssw_labels = df_concat.groupby(by='labels').apply(get_ss)  # compute ssw for each cluster labels\n",
    "        \n",
    "        \n",
    "        ssb = sst - np.sum(ssw_labels)  # remember: SST = SSW + SSB\n",
    "        \n",
    "        \n",
    "        r2.append(ssb / sst)  # save the R2 of the given cluster solution\n",
    "        \n",
    "    return np.array(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02835289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input\n",
    "hc_methods = [\"ward\", \"complete\", \"average\", \"single\"]\n",
    "# Call function defined above to obtain the R2 statistic for each hc_method\n",
    "max_nclus = 10\n",
    "r2_hc_methods = np.vstack(\n",
    "    [\n",
    "        get_r2_hc(df=df[metric_features], link_method=link, max_nclus=max_nclus) \n",
    "        for link in hc_methods\n",
    "    ]\n",
    ").T\n",
    "r2_hc_methods = pd.DataFrame(r2_hc_methods, index=range(1, max_nclus + 1), columns=hc_methods)\n",
    "\n",
    "sns.set()\n",
    "# Plot data\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "sns.lineplot(data=r2_hc_methods, linewidth=2.5, markers=[\"o\"]*4)\n",
    "\n",
    "# Finalize the plot\n",
    "fig.suptitle(\"R2 plot for various hierarchical methods\", fontsize=21)\n",
    "plt.gca().invert_xaxis()  # invert x axis\n",
    "plt.legend(title=\"HC methods\", title_fontsize=11)\n",
    "plt.xticks(range(1, max_nclus + 1))\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0aa86",
   "metadata": {},
   "source": [
    "### Defining the number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d092aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hclust = AgglomerativeClustering(linkage=linkage, affinity=distance, distance_threshold=0, n_clusters=None)\n",
    "hclust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de84ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 100\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a9f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 cluster solution\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hc4lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=4)\n",
    "hc4_labels = hc4lust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17803956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 4 clusters\n",
    "df_concat = pd.concat((df, pd.Series(hc4_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb52e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 cluster solution\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hc5lust = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=5)\n",
    "hc5_labels = hc5lust.fit_predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60229f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the 5 clusters\n",
    "df_concat = pd.concat((df, pd.Series(hc5_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f26c02",
   "metadata": {},
   "source": [
    "### K-Means clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust = KMeans(n_clusters=8, init='random', n_init=10, random_state=1)\n",
    "# the fit method\n",
    "kmclust.fit(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f494f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predict method\n",
    "kmclust.predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14a5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transform method\n",
    "pd.DataFrame(kmclust.transform(df[metric_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254334c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better initialization method and provide more n_init\n",
    "kmclust = KMeans(n_clusters=8, init='k-means++', n_init=15, random_state=1)\n",
    "kmclust.fit(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7102947",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmclust.predict(df[metric_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70adce",
   "metadata": {},
   "source": [
    "### Defining the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af849b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "for n_clus in range_clusters:  # iterate over desired ncluster range\n",
    "    kmclust = KMeans(n_clusters=n_clus, init='k-means++', n_init=15, random_state=1)\n",
    "    kmclust.fit(df[metric_features])\n",
    "    inertia.append(kmclust.inertia_)  # save the inertia of the given cluster solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(inertia)\n",
    "plt.ylabel(\"Inertia: SSw\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Inertia plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f53a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py\n",
    "\n",
    "# Storing average silhouette metric\n",
    "avg_silhouette = []\n",
    "for nclus in range_clusters:\n",
    "    # Skip nclus == 1\n",
    "    if nclus == 1:\n",
    "        continue\n",
    "    \n",
    "    # Create a figure\n",
    "    fig = plt.figure(figsize=(13, 7))\n",
    "\n",
    "    # Initialize the KMeans object with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    kmclust = KMeans(n_clusters=nclus, init='k-means++', n_init=15, random_state=1)\n",
    "    cluster_labels = kmclust.fit_predict(df[metric_features])\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed clusters\n",
    "    silhouette_avg = silhouette_score(df[metric_features], cluster_labels)\n",
    "    avg_silhouette.append(silhouette_avg)\n",
    "    print(f\"For n_clusters = {nclus}, the average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(df[metric_features], cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        # Get y_upper to demarcate silhouette y range size\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        # Filling the silhouette\n",
    "        color = cm.nipy_spectral(float(i) / nclus)\n",
    "        plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    plt.title(\"The silhouette plot for the various clusters.\")\n",
    "    plt.xlabel(\"The silhouette coefficient values\")\n",
    "    plt.ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    xmin, xmax = np.round(sample_silhouette_values.min() -0.1, 2), np.round(sample_silhouette_values.max() + 0.1, 2)\n",
    "    plt.xlim([xmin, xmax])\n",
    "    \n",
    "    # The (nclus+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    plt.ylim([0, len(df[metric_features]) + (nclus + 1) * 10])\n",
    "\n",
    "    plt.yticks([])  # Clear the yaxis labels / ticks\n",
    "    plt.xticks(np.arange(xmin, xmax, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average silhouette plot\n",
    "# The inertia plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(avg_silhouette)\n",
    "plt.ylabel(\"Average silhouette\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Average silhouette plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61cb928",
   "metadata": {},
   "source": [
    "### Final KMeans clustering solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190c319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cluster solution\n",
    "number_clusters = 7\n",
    "kmclust = KMeans(n_clusters=number_clusters, init='k-means++', n_init=15, random_state=1)\n",
    "km_labels = kmclust.fit_predict(df[metric_features])\n",
    "km_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e352d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characterizing the final clusters\n",
    "df_concat = pd.concat((df, pd.Series(km_labels, name='labels')), axis=1)\n",
    "df_concat.groupby('labels').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f1f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24d7834a",
   "metadata": {},
   "source": [
    "# Clustering by Perspectives\n",
    "- Demographic/Behavioral Perspective:\n",
    "- Product Perspective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8268614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split variables into perspectives (example, requires critical thinking and domain knowledge)\n",
    "demographic_features=[\n",
    "    'BirthYear',\n",
    "    'MonthSal',\n",
    "    'CustMonVal',\n",
    "    'ClaimsRate'\n",
    "]\n",
    "\n",
    "preference_features=[\n",
    "    'PremMotor', \n",
    "    'PremHousehold', \n",
    "    'PremHealth',\n",
    "    'PremLife', \n",
    "    'PremWork', \n",
    "]\n",
    "df_dem = df[demographic_features].copy()\n",
    "df_prf = df[preference_features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093e088",
   "metadata": {},
   "source": [
    "## Testing on K-means and Hierarchical clustering\n",
    "Based on (1) our previous tests and (2) the context of this problem, the optimal number of clusters is expected to be between 3 and 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee08fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust\n",
    "\n",
    "\n",
    "# Set up the clusterers\n",
    "kmeans = KMeans(\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "hierarchical = AgglomerativeClustering(\n",
    "    affinity='euclidean'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e3df0d",
   "metadata": {},
   "source": [
    "### Finding the optimal clusterer on demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa642bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on demographic variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans'] = get_r2_scores(df_dem, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_dem, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "pd.DataFrame(r2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fadad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the R² scores for each cluster solution on demographic variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Demographic Variables:\\nR² plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R² metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22569f",
   "metadata": {},
   "source": [
    "### Repeat the process for product variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534abe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the R² scores for each cluster solution on product variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans'] = get_r2_scores(df_prf, kmeans)\n",
    "\n",
    "for linkage in ['complete', 'average', 'single', 'ward']:\n",
    "    r2_scores[linkage] = get_r2_scores(\n",
    "        df_prf, hierarchical.set_params(linkage=linkage)\n",
    "    )\n",
    "\n",
    "# Visualizing the R² scores for each cluster solution on product variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    "plt.title(\"Product Variables:\\nR2 plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title=\"Cluster methods\", title_fontsize=11)\n",
    "plt.xlabel(\"Number of clusters\", fontsize=13)\n",
    "plt.ylabel(\"R2 metric\", fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885faeff",
   "metadata": {},
   "source": [
    "## Merging the Perspectives\n",
    "- How can we merge different cluster solutions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the right clustering (algorithm and number of clusters) for each perspective\n",
    "kmeans_prod = KMeans(\n",
    "    n_clusters=3\n",
    "    ,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "prod_labels = kmeans_prod.fit_predict(df_prf)\n",
    "\n",
    "kmeans_behav = KMeans(\n",
    "    n_clusters=3,\n",
    "    init='k-means++',\n",
    "    n_init=20,\n",
    "    random_state=42\n",
    ")\n",
    "behavior_labels = kmeans_behav.fit_predict(df_dem)\n",
    "\n",
    "df['product_labels'] = prod_labels\n",
    "df['behavior_labels'] = behavior_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f62da",
   "metadata": {},
   "source": [
    "### Merging using Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ccf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids of the concatenated cluster labels\n",
    "df_centroids = df.groupby(['behavior_labels', 'product_labels'])\\\n",
    "    [metric_features].mean()\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abbe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Hierarchical clustering to merge the concatenated cluster centroids\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    distance_threshold=0, \n",
    "    n_clusters=None\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddc2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 2.3\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels=df_centroids.index, p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b7223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(\n",
    "    linkage='ward', \n",
    "    affinity='euclidean', \n",
    "    n_clusters=3\n",
    ")\n",
    "hclust_labels = hclust.fit_predict(df_centroids)\n",
    "df_centroids['hclust_labels'] = hclust_labels\n",
    "\n",
    "df_centroids  # centroid's cluster labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36596888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = df_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "df_ = df.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "df_['merged_labels'] = df_.apply(\n",
    "    lambda row: cluster_mapper[\n",
    "        (row['behavior_labels'], row['product_labels'])\n",
    "    ], axis=1\n",
    ")\n",
    "\n",
    "# Merged cluster centroids\n",
    "df_.groupby('merged_labels').mean()[metric_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc536aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge cluster contigency table\n",
    "# Getting size of each final cluster\n",
    "df_counts = df_.groupby('merged_labels')\\\n",
    "    .size()\\\n",
    "    .to_frame()\n",
    "\n",
    "# Getting the product and behavior labels\n",
    "df_counts = df_counts\\\n",
    "    .rename({v:k for k, v in cluster_mapper.items()})\\\n",
    "    .reset_index()\n",
    "\n",
    "df_counts['behavior_labels'] = df_counts['merged_labels'].apply(lambda x: x[0])\n",
    "df_counts['product_labels'] = df_counts['merged_labels'].apply(lambda x: x[1])\n",
    "df_counts.pivot('behavior_labels', 'product_labels', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5430b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting df to have the final product, behavior and merged clusters\n",
    "df = df_.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fec308",
   "metadata": {},
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac19896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    df = df[metric_features.to_list() + ['product_labels', 'behavior_labels', 'merged_labels']], \n",
    "    label_columns = ['product_labels', 'behavior_labels', 'merged_labels'], \n",
    "    figsize = (28, 13), \n",
    "    compar_titles = [\"Product clustering\", \"Behavior clustering\", \"Merged clusters\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6741759f",
   "metadata": {},
   "source": [
    "## Cluster visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec988242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step can be quite time consuming\n",
    "two_dim = TSNE(random_state=42).fit_transform(df[metric_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c43856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c=df['merged_labels'], colormap='tab10', figsize=(15,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
